# PASO 1: IMPORTACI√ìN DE LIBRER√çAS
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import datetime as dt
import warnings
warnings.filterwarnings('ignore')

print("*IMPLEMENTACI√ìN DE MODELOS PREDICTIVOS PARA MINIMIZACI√ìN DE MERMAS*")
print("="*70)

# PASO 2: CARGA Y PREPARACI√ìN DE DATOS
# Cargar el dataset de mermas (ajustar nombre del archivo seg√∫n corresponda)
try:
    data = pd.read_csv('mermas_dataset.csv')  # Cambiar por el nombre real de tu archivo
    print(f"‚úì Dataset cargado exitosamente: {data.shape[0]} filas x {data.shape[1]} columnas")
    
except FileNotFoundError:
    print("‚ùå Error: No se encontr√≥ el archivo 'mermas_dataset.csv'")
    print("Por favor, aseg√∫rate de que el archivo est√© en el directorio correcto")
    exit()

# Filtrar registros con merma_monto > -100 FILTRASO
#data = data[data['merma_monto'] > -100]

#edicion de los signos

data['merma_monto_p'] = data['merma_monto_p'] * np.sign(data['merma_monto'])

# PASO 3: EXPLORACI√ìN INICIAL DEL DATASET DE MERMAS
print("\n=== EXPLORACI√ìN INICIAL DEL DATASET ===")
print("\nPrimeras 5 filas del dataset:")
print(data.head())

print("\nInformaci√≥n general del dataset:")
print(data.info())

print("\nEstad√≠sticas descriptivas de las variables objetivo:")
if 'merma_unidad' in data.columns:
    print(f"Merma en unidades - Estad√≠sticas:")
    print(data['merma_unidad'].describe())
    
if 'merma_monto' in data.columns:
    print(f"\nMerma en monto - Estad√≠sticas:")
    print(data['merma_monto'].describe())

# Verificar valores faltantes
print(f"\nValores faltantes por columna:")
missing_data = data.isnull().sum()
print(missing_data[missing_data > 0])

# PASO 4: PREPROCESAMIENTO DE FECHAS
print("\n=== PREPROCESAMIENTO DE FECHAS ===")

# Convertir fecha a datetime si existe
if 'fecha' in data.columns:
    try:
        # Intentar diferentes formatos de fecha
        data['fecha'] = pd.to_datetime(data['fecha'], errors='coerce')
        
        # Crear variables temporales adicionales
        data['dia_semana'] = data['fecha'].dt.dayofweek
        data['trimestre'] = data['fecha'].dt.quarter
        data['dia_mes'] = data['fecha'].dt.day
        
        print("‚úì Variables temporales creadas: dia_semana, trimestre, dia_mes")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Advertencia al procesar fechas: {e}")

# PASO 5: SELECCI√ìN DE VARIABLE OBJETIVO
print("\n=== SELECCI√ìN DE VARIABLE OBJETIVO ===")
target_variable = 'merma_monto'  # Definimos directamente la variable objetivo

# PASO 6: SELECCI√ìN DE CARACTER√çSTICAS PREDICTORAS
print("\n=== SELECCI√ìN DE CARACTER√çSTICAS PREDICTORAS ===")

# Corregir nombre de columna si es necesario
if 'ubicaci√≥n_motivo' in data.columns:
    data = data.rename(columns={'ubicaci√≥n_motivo': 'ubicacion_motivo'})

# Crear variables temporales adicionales
if 'fecha' in data.columns:
    try:
        data['mes_num'] = data['fecha'].dt.month
        print("‚úì Variables temporales adicionales creadas")
    except Exception as e:
        print(f"Error procesando fechas para 'mes_num': {e}")




# Variables categ√≥ricas
categorical_features = ['negocio', 'seccion', 'linea', 'categoria', 
                        'abastecimiento', 'comuna', 'region', 'tienda', 
                        'zonal', 'motivo', 'ubicacion_motivo', 'mes', 'semestre']

# Variables num√©ricas
numeric_features = ['a√±o', 'dia_semana', 'trimestre', 'dia_mes', 'mes_num']

# Filtrar solo las columnas que existen en el dataset
categorical_features = [col for col in categorical_features if col in data.columns]
numeric_features = [col for col in numeric_features if col in data.columns]

print(f"Caracter√≠sticas categ√≥ricas: {categorical_features}")
print(f"Caracter√≠sticas num√©ricas: {numeric_features}")

# Preparar datos para modelado
X = data[categorical_features + numeric_features]
y = data[target_variable]

# Eliminar filas con valores faltantes
mask = X.notna().all(axis=1)
X = X[mask]
y = y[mask]
data = data[mask] # Aseg√∫rate de que 'data' tambi√©n se filtre para mantener la correspondencia de √≠ndices

print(f"Registros finales para modelado: {len(X)}")

# PASO 7: DIVISI√ìN DE DATOS
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"\nConjunto de entrenamiento: {X_train.shape[0]} muestras")
print(f"Conjunto de prueba: {X_test.shape[0]} muestras")

# PASO 8: PREPROCESAMIENTO
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
    ]
)

print("‚úì Preprocesador configurado:")
print("   - Variables num√©ricas: Estandarizaci√≥n (StandardScaler)")
print("   - Variables categ√≥ricas: One-Hot Encoding")

# PASO 9: IMPLEMENTACI√ìN DE MODELOS
print("\n=== IMPLEMENTACI√ìN DE MODELOS ===")

# Modelo 1: Regresi√≥n Lineal (del ejemplo original)
pipeline_lr = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', LinearRegression())
])

# Modelo 2: Random Forest (del ejemplo original)
pipeline_rf = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))
])

# Modelo 3: Gradient Boosting (nuevo modelo adicional)
# Seleccionado porque es muy efectivo para problemas de regresi√≥n complejos
pipeline_gb = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', GradientBoostingRegressor(n_estimators=100, random_state=42))
])

# Modelo 4: K-Nearest Neighbors (del ejemplo original, opcional)
pipeline_knn = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', KNeighborsRegressor(n_neighbors=5, weights='distance'))
])

print("‚úì Modelos configurados:")
print("   1. Regresi√≥n Lineal")
print("   2. Random Forest")
print("   3. Gradient Boosting (nuevo)")
print("   4. K-Nearest Neighbors")

# PASO 10: ENTRENAMIENTO DE MODELOS
print("\n=== ENTRENAMIENTO DE MODELOS ===")

models = {
    'Regresi√≥n Lineal': pipeline_lr,
    'Random Forest': pipeline_rf,
    'Gradient Boosting': pipeline_gb,
    'K-Nearest Neighbors': pipeline_knn
}

trained_models = {}
for name, model in models.items():
    print(f"Entrenando {name}...")
    try:
        model.fit(X_train, y_train)
        trained_models[name] = model
        print(f"‚úì {name} entrenado exitosamente")
    except Exception as e:
        print(f"‚ùå Error entrenando {name}: {e}")

print(f"\n‚úì {len(trained_models)} modelos entrenados correctamente")

# PASO 11: EVALUACI√ìN DE MODELOS
print("\n=== EVALUACI√ìN DE MODELOS ===")

results = {}
predictions = {}

for name, model in trained_models.items():
    try:
        # Realizar predicciones
        y_pred = model.predict(X_test)
        predictions[name] = y_pred
        
        # Calcular m√©tricas
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        results[name] = {
            'MSE': mse,
            'RMSE': rmse,
            'MAE': mae,
            'R¬≤': r2
        }
        
        print(f"‚úì {name} evaluado")
        
    except Exception as e:
        print(f"‚ùå Error evaluando {name}: {e}")

# PASO 12: PRESENTACI√ìN DE RESULTADOS
print("\n=== RESULTADOS DE LA EVALUACI√ìN ===")

# Crear DataFrame con resultados
results_df = pd.DataFrame(results).T
results_df = results_df.round(4)

print("\nComparaci√≥n de m√©tricas entre modelos:")
print(results_df)

# Identificar el mejor modelo
best_model_name = results_df['R¬≤'].idxmax()
best_r2 = results_df.loc[best_model_name, 'R¬≤']
best_rmse = results_df.loc[best_model_name, 'RMSE']

print(f"\nüèÜ MEJOR MODELO: {best_model_name}")
print(f"   R¬≤ = {best_r2:.4f} (explica {best_r2*100:.1f}% de la variabilidad)")
print(f"   RMSE = {best_rmse:.2f} (error promedio de predicci√≥n)")

# PASO 13: AN√ÅLISIS ESPEC√çFICO PARA MERMAS
print("\n=== AN√ÅLISIS ESPEC√çFICO PARA GESTI√ìN DE MERMAS ===")

print("üéØ INTERPRETACI√ìN PARA MINIMIZACI√ìN DE MERMAS:")
print(f"‚Ä¢ El modelo {best_model_name} puede explicar {best_r2*100:.1f}% de la variaci√≥n en {target_variable}")
print(f"‚Ä¢ Error promedio de predicci√≥n: ¬±{best_rmse:.2f} unidades de {target_variable}")

# An√°lisis de distribuci√≥n de errores
best_predictions = predictions[best_model_name]
errors = y_test - best_predictions
relative_errors = (errors / y_test) * 100

print(f"\nüìä AN√ÅLISIS DE ERRORES DEL MEJOR MODELO:")
print(f"‚Ä¢ Error absoluto promedio: {np.mean(np.abs(errors)):.2f}")
print(f"‚Ä¢ Error relativo promedio: {np.mean(np.abs(relative_errors)):.1f}%")
print(f"‚Ä¢ Rango de errores: [{errors.min():.2f}, {errors.max():.2f}]")

# An√°lisis de impacto econ√≥mico potencial
if 'merma_monto' in target_variable:
    total_merma_real = y_test.sum()
    total_merma_predicha = best_predictions.sum()
    diferencia_total = abs(total_merma_real - total_merma_predicha)
    
    print(f"\nüí∞ IMPACTO ECON√ìMICO POTENCIAL:")
    print(f"‚Ä¢ Merma total real en test: ${total_merma_real:,.2f}")
    print(f"‚Ä¢ Merma total predicha: ${total_merma_predicha:,.2f}")
    print(f"‚Ä¢ Diferencia absoluta: ${diferencia_total:,.2f}")

# PASO 14: GUARDAR RESULTADOS DETALLADOS
print("\n=== GENERACI√ìN DE REPORTES ===")

# Crear DataFrame con predicciones detalladas
detailed_results = pd.DataFrame({
    'Valor_Real': y_test,
    'Mejor_Prediccion': best_predictions,
    'Error_Absoluto': np.abs(y_test - best_predictions),
    'Error_Relativo_Pct': np.abs((y_test - best_predictions) / y_test) * 100
})

# A√±adir informaci√≥n contextual
# Para asegurar que los √≠ndices de `X_test` y `data` se alineen correctamente
# crea una copia de `X_test` con el √≠ndice original para poder usarlo para lookup en `data`
X_test_original_index = X_test.index 
original_data_for_lookup = data.loc[X_test_original_index]

# Agregar variables categ√≥ricas m√°s importantes para contexto
# A√ëADO 'descripcion' AQU√ç

important_cats = ['categoria', 'motivo', 'region', 'tienda', 'negocio', 'linea', 'descripcion']

for cat in important_cats:
    if cat in original_data_for_lookup.columns:
        detailed_results[cat] = original_data_for_lookup[cat].values
    else:
        print(f"Columna '{cat}' no encontrada en original_data_for_lookup.")

# --- MODIFICACI√ìN CLAVE AQU√ç PARA A√ëADIR LA DESCRIPCI√ìN ---
# Generar una columna de descripci√≥n combinando variables clave
# AHORA INCLUYE 'descripcion' DEL PRODUCTO
detailed_results['Descripcion_Contextual'] = detailed_results.apply(
    lambda row: f"Merma de '{row.get('descripcion', 'N/A')}' (Cat: {row.get('categoria', 'N/A')}) por {row.get('motivo', 'N/A')} en {row.get('tienda', 'N/A')}.", axis=1
)
# Puedes ajustar esta lambda function para construir la descripci√≥n como prefieras
# Por ejemplo, puedes a√±adir:
# f"Negocio: {row.get('negocio', 'N/A')}, L√≠nea: {row.get('linea', 'N/A')}"
# Aseg√∫rate de que las columnas que usas existan en detailed_results despu√©s de la asignaci√≥n anterior.
# -------------------------------------------------------------

# Ordenar por error para identificar casos problem√°ticos
detailed_results = detailed_results.sort_values('Error_Absoluto', ascending=False)

# Guardar reporte principal
with open('reporte_prediccion_mermas.md', 'w', encoding='utf-8') as f:
    f.write('# Reporte de Predicci√≥n de Mermas\n\n')
    
    f.write('## Resumen Ejecutivo\n\n')
    f.write(f'Se evaluaron {len(trained_models)} modelos de machine learning para predecir {target_variable}. ')
    f.write(f'El mejor modelo fue **{best_model_name}** con un R¬≤ de {best_r2:.4f}.\n\n')
    
    f.write('## Comparaci√≥n de Modelos\n\n')
    f.write('| Modelo | R¬≤ | RMSE | MAE | MSE |\n')
    f.write('|--------|----|----- |-----|-----|\n')
    for model_name, metrics in results.items():
        f.write(f"| {model_name} | {metrics['R¬≤']:.4f} | {metrics['RMSE']:.2f} | {metrics['MAE']:.2f} | {metrics['MSE']:.2f} |\n")
    
    f.write(f'\n## Mejor Modelo: {best_model_name}\n\n')
    f.write(f'### Capacidad Predictiva\n')
    f.write(f'- **R¬≤**: {best_r2:.4f} (explica {best_r2*100:.1f}% de la variabilidad)\n')
    f.write(f'- **RMSE**: {best_rmse:.2f} (error promedio de predicci√≥n)\n')
    f.write(f'- **Error relativo promedio**: {np.mean(np.abs(relative_errors)):.1f}%\n\n')
    
    f.write('### Implicaciones para Gesti√≥n de Mermas\n\n')
    f.write('**Beneficios del modelo:**\n')
    f.write('- Permite anticipar niveles de merma antes de que ocurran\n')
    f.write('- Facilita la planificaci√≥n de inventarios m√°s precisa\n')
    f.write('- Identifica patrones de merma por categor√≠a, regi√≥n y motivo\n')
    f.write('- Habilita estrategias preventivas de reducci√≥n de p√©rdidas\n\n')
    
    f.write('**Casos de mayor error (Top 10):**\n\n')
    # RENOMBRAMOS LA COLUMNA A 'Descripci√≥n Detallada'
    f.write('| # | Valor Real | Predicci√≥n | Error | Error % | Contexto | Descripci√≥n Detallada |\n') 
    f.write('|---|------------|------------|-------|---------|----------|-----------------------|\n') # Ajuste para nueva columna
    
    for i, (idx, row) in enumerate(detailed_results.head(10).iterrows()):
        contexto = []
        # Eliminamos 'descripcion' de 'contexto_str' para que no se duplique con 'Descripcion_Contextual'
        # Aqu√≠ incluimos solo las categor√≠as principales o de ubicaci√≥n.
        context_cols_for_table = ['categoria', 'motivo', 'region', 'tienda'] 
        for cat in context_cols_for_table: 
            if cat in row.index: # Asegurarse de que la columna existe en la fila
                contexto.append(f"{cat}: {row[cat]}")
        contexto_str = " | ".join(contexto) # Puedes ajustar el n√∫mero de elementos aqu√≠
        
        # --- MODIFICACI√ìN CLAVE AQU√ç PARA MOSTRAR LA DESCRIPCI√ìN ---
        f.write(f"| {i+1} | {row['Valor_Real']:.2f} | {row['Mejor_Prediccion']:.2f} | ")
        f.write(f"{row['Error_Absoluto']:.2f} | {row['Error_Relativo_Pct']:.1f}% | {contexto_str} | {row['Descripcion_Contextual']} |\n")
        # -------------------------------------------------------------
    
    f.write('\n## Recomendaciones\n\n')
    f.write('### Implementaci√≥n Inmediata\n')
    f.write(f'1. **Usar {best_model_name}** como modelo principal de predicci√≥n\n')
    f.write('2. **Monitorear errores** especialmente en casos con error >50%\n')
    f.write('3. **Validar predicciones** con expertos del negocio\n\n')
    
    f.write('### Mejoras Futuras\n')
    f.write('1. **Incorporar variables adicionales**: clima, promociones, d√≠as festivos\n')
    f.write('2. **Implementar validaci√≥n temporal**: entrenar con datos hist√≥ricos\n')
    ("3. Desarrollar modelos espec√≠ficos por categor√≠a de producto\n")
    f.write('4. **Establecer alertas autom√°ticas** para predicciones inusuales\n\n')
    
    f.write('*Reporte generado autom√°ticamente por el sistema de an√°lisis predictivo*\n')

print("‚úì Reporte principal guardado: reporte_prediccion_mermas.md")

# PASO 15: VISUALIZACIONES
print("\n=== GENERACI√ìN DE VISUALIZACIONES ===")

try:
    # Configurar estilo
    plt.style.use('default')
    sns.set_palette("husl")
    
    # Gr√°fico 1: Comparaci√≥n de modelos
    fig, ax = plt.subplots(1, 1, figsize=(10, 6))
    models_names = list(results.keys())
    r2_scores = [results[name]['R¬≤'] for name in models_names]
    
    bars = ax.bar(models_names, r2_scores, color=['skyblue', 'lightgreen', 'coral', 'gold'])
    ax.set_ylabel('R¬≤ Score')
    ax.set_title('Comparaci√≥n de Modelos - Capacidad Predictiva (R¬≤)')
    ax.set_ylim(0, max(r2_scores) * 1.1)
    
    # A√±adir valores en las barras
    for bar, score in zip(bars, r2_scores):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{score:.3f}', ha='center', va='bottom')
    
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig('comparacion_modelos_mermas.png', dpi=300, bbox_inches='tight')
    print("‚úì Gr√°fico guardado: comparacion_modelos_mermas.png")
    
    # Gr√°fico 2: Predicciones vs Valores Reales (mejor modelo)
    fig, ax = plt.subplots(1, 1, figsize=(10, 8))
    ax.scatter(y_test, best_predictions, alpha=0.6, color='blue', s=50)
    
    # L√≠nea diagonal perfecta
    min_val = min(y_test.min(), best_predictions.min())
    max_val = max(y_test.max(), best_predictions.max())
    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Predicci√≥n Perfecta')
    
    ax.set_xlabel(f'{target_variable} - Valores Reales')
    ax.set_ylabel(f'{target_variable} - Predicciones')
    ax.set_title(f'Predicciones vs Valores Reales - {best_model_name}\n(R¬≤ = {best_r2:.4f})')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('predicciones_vs_reales_mermas.png', dpi=300, bbox_inches='tight')
    print("‚úì Gr√°fico guardado: predicciones_vs_reales_mermas.png")
    
    # Gr√°fico 3: Distribuci√≥n de errores
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Histograma de errores absolutos
    ax1.hist(np.abs(errors), bins=30, alpha=0.7, color='coral', edgecolor='black')
    ax1.set_xlabel('Error Absoluto')
    ax1.set_ylabel('Frecuencia')
    ax1.set_title('Distribuci√≥n de Errores Absolutos')
    ax1.grid(True, alpha=0.3)
    
    # Histograma de errores relativos
    ax2.hist(np.abs(relative_errors), bins=30, alpha=0.7, color='lightgreen', edgecolor='black')
    ax2.set_xlabel('Error Relativo (%)')
    ax2.set_ylabel('Frecuencia')
    ax2.set_title('Distribuci√≥n de Errores Relativos')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('distribucion_errores_mermas.png', dpi=300, bbox_inches='tight')
    print("‚úì Gr√°fico guardado: distribucion_errores_mermas.png")
    
except Exception as e:
    print(f"‚ö†Ô∏è Advertencia al generar gr√°ficos: {e}")

# PASO 16: AN√ÅLISIS DE IMPORTANCIA DE CARACTER√çSTICAS
print("\n=== AN√ÅLISIS DE IMPORTANCIA DE CARACTER√çSTICAS ===")

if best_model_name == 'Random Forest' or best_model_name == 'Gradient Boosting':
    try:
        # Obtener modelo entrenado
        best_model = trained_models[best_model_name]
        regressor = best_model.named_steps['regressor']
        
        if hasattr(regressor, 'feature_importances_'):
            # Obtener nombres de caracter√≠sticas despu√©s del preprocesamiento
            preprocessor = best_model.named_steps['preprocessor']
            
            # Caracter√≠sticas num√©ricas (mantienen sus nombres)
            num_feature_names = numeric_features
            
            # Caracter√≠sticas categ√≥ricas (se expanden con one-hot encoding)
            if len(categorical_features) > 0:
                cat_transformer = preprocessor.named_transformers_['cat']
                if hasattr(cat_transformer, 'get_feature_names_out'):
                    cat_feature_names = cat_transformer.get_feature_names_out(categorical_features)
                else:
                    # Fallback si get_feature_names_out no est√° disponible (menos com√∫n ahora)
                    # Es una aproximaci√≥n, puede no ser exacta si hay muchos valores √∫nicos
                    print("‚ö†Ô∏è Advertencia: get_feature_names_out no disponible. Usando nombres de caracter√≠sticas aproximados para categ√≥ricas.")
                    # Generar nombres de caracter√≠sticas sint√©ticos, esto es un placeholder
                    # Necesitar√≠as obtener los valores √∫nicos de cada columna categ√≥rica para generar nombres m√°s precisos.
                    all_transformed_features = preprocessor.fit_transform(X_train).shape[1]
                    cat_feature_names = [f"cat_feat_{i}" for i in range(all_transformed_features - len(numeric_features))]
            else:
                cat_feature_names = []
            
            # Combinar nombres de caracter√≠sticas
            all_feature_names = list(num_feature_names) + list(cat_feature_names)
            
            # Obtener importancias
            importances = regressor.feature_importances_
            
            # Crear DataFrame con importancias
            if len(all_feature_names) == len(importances):
                feature_importance_df = pd.DataFrame({
                    'caracteristica': all_feature_names,
                    'importancia': importances
                }).sort_values('importancia', ascending=False)
                
                print("‚úì Top 15 caracter√≠sticas m√°s importantes:")
                print(feature_importance_df.head(15))
                
                # Guardar an√°lisis de importancia
                with open('importancia_caracteristicas_mermas.md', 'w', encoding='utf-8') as f:
                    f.write('# An√°lisis de Importancia de Caracter√≠sticas\n\n')
                    f.write(f'## Modelo: {best_model_name}\n\n')
                    f.write('### Top 15 Caracter√≠sticas M√°s Importantes\n\n')
                    f.write('| Ranking | Caracter√≠stica | Importancia | Interpretaci√≥n |\n')
                    f.write('|---------|----------------|-------------|----------------|\n')
                    
                    for i, (_, row) in enumerate(feature_importance_df.head(15).iterrows(), 1):
                        interpretacion = "Variable num√©rica" if row['caracteristica'] in numeric_features else "Variable categ√≥rica"
                        f.write(f"| {i} | {row['caracteristica']} | {row['importancia']:.4f} | {interpretacion} |\n")
                    
                    f.write('\n### Insights para Gesti√≥n de Mermas\n\n')
                    f.write('Las caracter√≠sticas m√°s importantes identificadas por el modelo indican:\n\n')
                    
                    top_features = feature_importance_df.head(5)['caracteristica'].tolist()
                    for i, feature in enumerate(top_features, 1):
                        # Intentar una interpretaci√≥n m√°s detallada si la caracter√≠stica es una categor√≠a o un one-hot encoding
                        if any(cat_col in feature for cat_col in categorical_features):
                            original_cat_col = next((cat_col for cat_col in categorical_features if cat_col in feature), "Categor√≠a")
                            f.write(f'{i}. **{feature}**: Una de las categor√≠as importantes de **{original_cat_col}** que influye significativamente en la merma.\n')
                        elif any(temp in feature.lower() for temp in ['mes', 'a√±o', 'trimestre', 'dia_semana']):
                            f.write(f'{i}. **{feature}**: Factor temporal significativo que puede indicar estacionalidad o patrones diarios/mensuales de merma.\n')
                        else:
                            f.write(f'{i}. **{feature}**: Factor operacional importante que requiere atenci√≥n para reducir mermas.\n')
                    
                    f.write('\n*Este an√°lisis ayuda a enfocar esfuerzos de reducci√≥n de mermas en los factores m√°s influyentes.*\n')
                
                print("‚úì An√°lisis de importancia guardado: importancia_caracteristicas_mermas.md")
                
            else:
                print("‚ö†Ô∏è No se pudo generar an√°lisis de importancia por diferencias dimensionales entre las caracter√≠sticas y las importancias.")
                print(f"N√∫mero de nombres de caracter√≠sticas esperados: {len(all_feature_names)}")
                print(f"N√∫mero de importancias obtenidas: {len(importances)}")
                
        else:
            print(f"‚ö†Ô∏è El modelo {best_model_name} no tiene el atributo 'feature_importances_'.")
            
    except Exception as e:
        print(f"‚ö†Ô∏è Error en an√°lisis de importancia: {e}")

# PASO 17: RESUMEN FINAL Y RECOMENDACIONES
print("\n" + "="*70)
print("üéØ RESUMEN FINAL - AN√ÅLISIS PREDICTIVO DE MERMAS")
print("="*70)

print(f"\nüìä MEJORES RESULTADOS:")
print(f"‚Ä¢ Modelo √≥ptimo: {best_model_name}")
print(f"‚Ä¢ Capacidad predictiva: {best_r2:.1%} de la variabilidad explicada")
print(f"‚Ä¢ Error promedio: ¬±{best_rmse:.2f} unidades de {target_variable}")
print(f"‚Ä¢ Total de registros analizados: {len(X)}")

print(f"\nüí° APLICACI√ìN PR√ÅCTICA:")
print("1. üîÆ Predicci√≥n proactiva de mermas futuras")
print("2. üìà Optimizaci√≥n de niveles de inventario")
print("3. üéØ Identificaci√≥n de productos/ubicaciones de alto riesgo")
print("4. üí∞ Estimaci√≥n de impacto econ√≥mico de p√©rdidas")

print(f"\nüöÄ PR√ìXIMOS PASOS RECOMENDADOS:")
print("1. Implementar el modelo en ambiente de producci√≥n")
print("2. Establecer monitoreo continuo de performance")
print("3. Desarrollar dashboards de alertas tempranas")
print("4. Capacitar equipos en interpretaci√≥n de resultados")
print("5. Iterar y mejorar el modelo con nuevos datos")

print(f"\nüìÅ ARCHIVOS GENERADOS:")
print("‚Ä¢ reporte_prediccion_mermas.md (reporte principal)")
print("‚Ä¢ comparacion_modelos_mermas.png (comparaci√≥n visual)")
print("‚Ä¢ predicciones_vs_reales_mermas.png (validaci√≥n del modelo)")
print("‚Ä¢ distribucion_errores_mermas.png (an√°lisis de errores)")
if best_model_name in ['Random Forest', 'Gradient Boosting']:
    print("‚Ä¢ importancia_caracteristicas_mermas.md (factores clave)")

print(f"\n‚úÖ AN√ÅLISIS PREDICTIVO DE MERMAS COMPLETADO EXITOSAMENTE")
print("="*70)

# Nota final sobre adaptaciones espec√≠ficas
print(f"\nüìã NOTA SOBRE ADAPTACIONES REALIZADAS:")
print("‚Ä¢ ‚úì Cambio de objetivo: de maximizaci√≥n de ventas a minimizaci√≥n de mermas")
print("‚Ä¢ ‚úì Variables adaptadas: uso de campos espec√≠ficos del dataset de mermas")
print("‚Ä¢ ‚úì Modelo adicional: Gradient Boosting para mejor capacidad predictiva")
print("‚Ä¢ ‚úì M√©tricas contextualizadas: interpretaci√≥n desde perspectiva de p√©rdidas")
print("‚Ä¢ ‚úì An√°lisis de impacto: consideraci√≥n del costo econ√≥mico de errores")
print("‚Ä¢ ‚úì Descripci√≥n contextual a√±adida en el reporte de errores para mayor claridad, incluyendo la descripci√≥n del producto.")
